{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FmkKLHM0_WIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c74f30-12bf-46a5-a48c-011680368d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "id": "TlU03_5vBCqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install timm\n"
      ],
      "metadata": {
        "id": "ep7tIMICBFAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import os"
      ],
      "metadata": {
        "id": "hqcsV-6MBTlq"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PairedPackageDataset(Dataset):\n",
        "    def __init__(self, side_paths, top_paths, labels, transform=None):\n",
        "        self.side_paths = side_paths\n",
        "        self.top_paths = top_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        side_img = Image.open(self.side_paths[idx]).convert(\"RGB\")\n",
        "        top_img = Image.open(self.top_paths[idx]).convert(\"RGB\")\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        if self.transform:\n",
        "            side_img = self.transform(side_img)\n",
        "            top_img = self.transform(top_img)\n",
        "        return (side_img, top_img), label"
      ],
      "metadata": {
        "id": "D4y6z90uBUdc"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "dMAQVaPeC18K"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminhos\n",
        "damage_top = sorted(glob('/content/drive/MyDrive/Image-Classification-in-Production-line/damaged/top/*.png'))\n",
        "damage_side = sorted(glob('/content/drive/MyDrive/Image-Classification-in-Production-line/damaged/side/*.png'))\n",
        "intact_top = sorted(glob('/content/drive/MyDrive/Image-Classification-in-Production-line/intact/top/*.png'))\n",
        "intact_side = sorted(glob('/content/drive/MyDrive/Image-Classification-in-Production-line/intact/side/*.png'))\n",
        "\n",
        "# Labels\n",
        "damage_labels = [1] * len(damage_top)\n",
        "intact_labels = [0] * len(intact_top)\n",
        "\n",
        "# Combine\n",
        "all_top = damage_top + intact_top\n",
        "all_side = damage_side + intact_side\n",
        "all_labels = damage_labels + intact_labels\n",
        "\n",
        "# Shuffle\n",
        "all_top, all_side, all_labels = shuffle(all_top, all_side, all_labels, random_state=42)\n",
        "\n",
        "# Split\n",
        "top_train, top_val, side_train, side_val, y_train, y_val = train_test_split(\n",
        "    all_top, all_side, all_labels, test_size=0.2, stratify=all_labels, random_state=42\n",
        ")\n",
        "\n",
        "# Datasets e Loaders\n",
        "train_dataset = PairedPackageDataset(side_train, top_train, y_train, transform=train_transform)\n",
        "val_dataset = PairedPackageDataset(side_val, top_val, y_val, transform=train_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n"
      ],
      "metadata": {
        "id": "IOOlNcKbC4jL"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet18\n",
        "\n",
        "class DualResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.backbone = resnet18(pretrained=True)\n",
        "        self.backbone.fc = nn.Identity()\n",
        "        self.side_branch = self.backbone\n",
        "        self.top_branch = resnet18(pretrained=True)\n",
        "        self.top_branch.fc = nn.Identity()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 2, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        side_img, top_img = inputs\n",
        "        side_feat = self.side_branch(side_img)\n",
        "        top_feat = self.top_branch(top_img)\n",
        "        x = torch.cat((side_feat, top_feat), dim=1)\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "v-cx4-eAEFFy"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicialização\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DualResNet().to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "# Treinamento\n",
        "best_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "max_epochs_stop = 5\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for (x_pair, labels) in train_loader:\n",
        "        x_pair = (x_pair[0].to(device), x_pair[1].to(device))\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_pair)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "\n",
        "    # Avaliação\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for (x_pair, labels) in val_loader:\n",
        "            x_pair = (x_pair[0].to(device), x_pair[1].to(device))\n",
        "            labels = labels.to(device).unsqueeze(1)\n",
        "            outputs = model(x_pair)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            predicted = (preds > 0.5).float()\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uggmhu_vEGM8",
        "outputId": "d54fc835-63c3-4086-bce2-c180b247d8cc"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 0.6636 | Val Loss: 0.7578 | Val Acc: 0.5000\n",
            "Epoch 2 | Train Loss: 0.6963 | Val Loss: 0.6855 | Val Acc: 0.4444\n",
            "Epoch 3 | Train Loss: 0.6320 | Val Loss: 0.6609 | Val Acc: 0.5556\n",
            "Epoch 4 | Train Loss: 0.5345 | Val Loss: 0.5511 | Val Acc: 0.7222\n",
            "Epoch 5 | Train Loss: 0.4650 | Val Loss: 0.5687 | Val Acc: 0.6944\n",
            "Epoch 6 | Train Loss: 0.3454 | Val Loss: 0.6726 | Val Acc: 0.8056\n",
            "Epoch 7 | Train Loss: 0.3299 | Val Loss: 0.5584 | Val Acc: 0.8333\n",
            "Epoch 8 | Train Loss: 0.3457 | Val Loss: 0.7574 | Val Acc: 0.6667\n",
            "Epoch 9 | Train Loss: 0.2052 | Val Loss: 0.4548 | Val Acc: 0.8333\n",
            "Epoch 10 | Train Loss: 0.2066 | Val Loss: 0.5442 | Val Acc: 0.7222\n",
            "Epoch 11 | Train Loss: 0.1642 | Val Loss: 0.5605 | Val Acc: 0.7500\n",
            "Epoch 12 | Train Loss: 0.2617 | Val Loss: 0.3862 | Val Acc: 0.8333\n",
            "Epoch 13 | Train Loss: 0.1640 | Val Loss: 0.4169 | Val Acc: 0.8333\n",
            "Epoch 14 | Train Loss: 0.1308 | Val Loss: 0.5087 | Val Acc: 0.8056\n",
            "Epoch 15 | Train Loss: 0.1365 | Val Loss: 0.8584 | Val Acc: 0.7778\n",
            "Epoch 16 | Train Loss: 0.0961 | Val Loss: 0.5662 | Val Acc: 0.8056\n",
            "Epoch 17 | Train Loss: 0.0931 | Val Loss: 0.4724 | Val Acc: 0.8056\n",
            "Epoch 18 | Train Loss: 0.0904 | Val Loss: 0.3770 | Val Acc: 0.8889\n",
            "Epoch 19 | Train Loss: 0.0893 | Val Loss: 0.5948 | Val Acc: 0.8611\n",
            "Epoch 20 | Train Loss: 0.0842 | Val Loss: 0.6780 | Val Acc: 0.8056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Métricas Finais\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "model.eval()\n",
        "preds, targets = [], []\n",
        "with torch.no_grad():\n",
        "    for (side_img, top_img), labels in val_loader:\n",
        "        side_img, top_img = side_img.to(device), top_img.to(device)\n",
        "        outputs = model((side_img, top_img))\n",
        "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "        pred = (probs > 0.5).astype(int).flatten()\n",
        "        preds.extend(pred)\n",
        "        targets.extend(labels.numpy())\n",
        "\n",
        "print(\"F1:\", f1_score(targets, preds))\n",
        "print(\"Precision:\", precision_score(targets, preds))\n",
        "print(\"Recall:\", recall_score(targets, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCtJbKdgEHVE",
        "outputId": "f5ab8203-8184-492b-f104-7ad12d24ac71"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.8333333333333334\n",
            "Precision: 0.8333333333333334\n",
            "Recall: 0.8333333333333334\n"
          ]
        }
      ]
    }
  ]
}